import os
import sys
import nest_asyncio
nest_asyncio.apply()

import warnings
import logging
import asyncio
import torch
import re

warnings.filterwarnings("ignore")
os.environ["TRANSFORMERS_VERBOSITY"] = "error"  # åªæ˜¾ç¤ºä¸¥é‡é”™è¯¯
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1" # å±è”½ Windows ä¸‹çš„ç¬¦å·é“¾æ¥è­¦å‘Š
logging.getLogger("langchain").setLevel(logging.ERROR)
logging.getLogger("langgraph").setLevel(logging.ERROR)

from typing import Annotated, Literal, TypedDict

# --- LlamaIndex ä¾èµ– (ç”¨äº RAG) ---
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.elasticsearch import ElasticsearchStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker
from llama_index.llms.openai_like import OpenAILike

# --- LangGraph & LangChain ä¾èµ– (ç”¨äº Agent) ---
from langgraph.graph import StateGraph
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv(override=True)

# ==============================================================================
# 1. å‡†å¤‡ RAG å¼•æ“
# ==============================================================================

# é…ç½® Embedding
GLOBAL_EMBED_MODEL = HuggingFaceEmbedding(model_name="BAAI/bge-m3")

Settings.embed_model = GLOBAL_EMBED_MODEL

# é…ç½®llm
Settings.llm = OpenAILike(
        model="deepseek-chat",
        api_key=os.getenv('DEEPSEEK_API_KEY'),
        api_base="https://api.deepseek.com",
        is_chat_model=True,
        context_window=32768,
        temperature=0.1,
        max_tokens=1024
    )

# é…ç½® Reranker (æ ¸å¿ƒç«äº‰åŠ›: é‡æ’åº)
reranker = FlagEmbeddingReranker(
    model="BAAI/bge-reranker-base", 
    top_n=5,
    use_fp16=True  # å¿…é¡»å¼€å¯åŠç²¾åº¦ï¼Œè¿›ä¸€æ­¥çœæ˜¾å­˜
)

# ==============================================================================
# 2. å®šä¹‰ Agent çš„å·¥å…· (Tool)
# ==============================================================================

@tool
def search_factory_knowledge(query: str) -> str:
    """
    å½“ç”¨æˆ·è¯¢é—®å·¥å‚è®¾å¤‡æ•…éšœã€é”™è¯¯ç ã€ç»´ä¿®æ­¥éª¤æˆ–æ“ä½œè§„ç¨‹æ—¶ï¼Œå¿…é¡»è°ƒç”¨æ­¤å·¥å…·è¿›è¡ŒæŸ¥è¯¢ã€‚
    é‡è¦æç¤º:query å‚æ•°å¿…é¡»æ˜¯å®Œæ•´çš„ä¸­æ–‡é—®é¢˜å¥å­ï¼Œä¸è¦éšæ„å¯¹ç”¨æˆ·çš„é—®é¢˜è¿›è¡Œæ¦‚æ‹¬ã€ä¸è¦æå–å…³é”®è¯ã€‚
    :param query: å¿…è¦å‚æ•°ï¼Œå­—ç¬¦ä¸²ç±»å‹ï¼Œç”¨äºè¾“å…¥ç”¨æˆ·çš„å…·ä½“é—®é¢˜ã€‚
    :return: è¿”å›æŸ¥è¯¢çš„ç»“æœå’Œæ¥æºæ–‡ä»¶ï¼ŒåŒ…å«æ–‡æœ¬å’Œå¯¹åº”å›¾ç‰‡é“¾æ¥çš„ç»“æ„åŒ–ç»“æœã€‚
    """
    print(f"\nğŸ” [Agent åŠ¨ä½œ] æ­£åœ¨è°ƒç”¨çŸ¥è¯†åº“æŸ¥è¯¢: {query}")
    vector_store = None
    try:
        # è¿æ¥ ES æ•°æ®åº“
        vector_store = ElasticsearchStore(
            es_url="http://localhost:9200",
            index_name="factory_knowledge",
        )
        index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

        # RAG Engine
        rag_engine = index.as_query_engine(
            similarity_top_k=15,  # ç²—æ’
            node_postprocessors=[reranker], # ç²¾æ’
            verbose=True
        )
        # è°ƒç”¨ LlamaIndex çš„ RAG å¼•æ“
        response = rag_engine.query(query)
        final_context_list = []
        
        if hasattr(response, 'source_nodes'):
            for node in response.source_nodes:
                text = node.text
                fname = node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                page = node.metadata.get('page_label', 'æœªçŸ¥é¡µ')
                
                # ç›´æ¥ä¿¡ä»» metadata
                # å› ä¸º kb_manager å·²ç»å¸®æˆ‘ä»¬æŠŠå›¾ç‰‡æŒ‰é¡µåˆ†å¥½äº†
                images = node.metadata.get("image_files", [])
                
                # ç»„è£…æ–‡æœ¬
                context_str = f"--- æ¥æº: {fname} (ç¬¬ {page} é¡µ) ---\n{text}\n"
                
                # ç»„è£…å›¾ç‰‡ (ç´§è·Ÿåœ¨æ–‡æœ¬åé¢)
                # è¿™æ · LLM è¯»åˆ°è¿™æ®µè¯æ—¶ï¼Œé©¬ä¸Šå°±èƒ½çœ‹åˆ°å›¾ç‰‡ï¼Œä»è€Œåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶å®ç°å›¾æ–‡ç©¿æ’
                if images and isinstance(images, list):
                    context_str += "\n[è¯¥æ®µè½å…³è”å‚è€ƒå›¾]:\n"
                    for img_name in images:
                        img_url = f"http://localhost:8000/images/{img_name}"
                        context_str += f"![ç¤ºæ„å›¾]({img_url})\n"
                
                final_context_list.append(context_str)

        final_response = "\n\n".join(final_context_list)
        
        if not final_response:
            return "æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"

        # Debug æ‰“å°
        if "![ç¤ºæ„å›¾]" in final_response:
             print("âœ… [Debug] æˆåŠŸæ£€æµ‹åˆ°å…³è”å›¾ç‰‡ï¼Œå·²æ³¨å…¥ä¸Šä¸‹æ–‡")
        
        return final_response
    except Exception as e:
        print(f"âŒ è¯¦ç»†é”™è¯¯: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return f"æŸ¥è¯¢å‡ºé”™: {e}"
    
    finally:
        # æ˜¾å¼å…³é—­ Elasticsearch å®¢æˆ·ç«¯è¿æ¥
        if vector_store is not None:
            try:
                # å…³é—­ ES å®¢æˆ·ç«¯
                if hasattr(vector_store, 'client'):
                    asyncio.get_event_loop().run_until_complete(
                        vector_store.client.close()
                    )
            except Exception as e:
                pass  # å¿½ç•¥å…³é—­æ—¶çš„é”™è¯¯

# å·¥å…·åˆ—è¡¨
tools = [search_factory_knowledge]

# ==============================================================================
# 3. æ„å»ºAgent
# ==============================================================================

llm = ChatOpenAI(
    model="deepseek-chat",
    openai_api_key=os.getenv('DEEPSEEK_API_KEY'),
    openai_api_base="https://api.deepseek.com",
    temperature=0.1
)

system_prompt = SystemMessage(content="""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥å‚æ™ºèƒ½åŠ©æ‰‹ã€‚
    1. é‡åˆ°æ•…éšœé—®é¢˜æˆ–æ“ä½œé—®é¢˜ï¼Œå¿…é¡»ä¼˜å…ˆä½¿ç”¨ 'search_factory_knowledge' å·¥å…·æŸ¥è¯¢çŸ¥è¯†åº“ã€‚
    2. å¦‚æœæŸ¥è¯¢å·¥å…·è¿”å›äº†è§£å†³æ–¹æ³•æˆ–æ“ä½œæ­¥éª¤ï¼Œè¯·æ¸…æ™°åœ°è½¬è¿°ç»™ç”¨æˆ·ï¼Œå¹¶å‘Šè¯‰ç”¨æˆ·å‚è€ƒæ¥æºæ–‡ä»¶çš„åç§°ã€‚
    3. å¦‚æœå·¥å…·è¿”å›çš„å†…å®¹ä¸­åŒ…å«å›¾ç‰‡é“¾æ¥ï¼ˆMarkdownæ ¼å¼å¦‚ ![](http://...)ï¼‰ï¼Œè¯·åŠ¡å¿…åœ¨å›ç­”çš„å¯¹åº”ä½ç½®åŸæ ·å±•ç¤ºè¿™äº›å›¾ç‰‡ï¼Œä¸è¦å¿½ç•¥å®ƒä»¬ï¼Œä¹Ÿä¸è¦ä¿®æ”¹é“¾æ¥åœ°å€ã€‚å›¾ç‰‡å¯¹äºç”¨æˆ·ç†è§£æ“ä½œæ­¥éª¤éå¸¸é‡è¦ã€‚
    4. ä¸å…è®¸åœ¨æŸ¥è¯¢å·¥å…·è¿”å›çš„å†…å®¹ä¸Šå¢åŠ æ— ä¸­ç”Ÿæœ‰çš„å†…å®¹ï¼Œä½ åªèƒ½å¯¹æŸ¥è¯¢çš„ç»“æœè¿›è¡Œæ•´åˆå¹¶æ¸…æ™°åœ°å›ç­”ç”¨æˆ·ã€‚
    5. å¦‚æœæŸ¥è¯¢ç»“æœä¸è¶³ä»¥æ”¯æ’‘ä½ å›ç­”ç”¨æˆ·çš„é—®é¢˜æˆ–ä¸ç”¨æˆ·çš„é—®é¢˜å…³è”æ€§å¾ˆå°ï¼Œè¯·ä½ è¯šå®å›ç­”æŸ¥è¯¢ä¸åˆ°ç›¸å…³ç»“æœï¼Œæˆ–è¿½é—®ç”¨æˆ·é—®é¢˜çš„ç»†èŠ‚ã€‚
    6. å¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸æ¸…æ™°ï¼ˆä¾‹å¦‚åªè¯´äº†â€œæœºå™¨åäº†â€ï¼‰ï¼Œè¯·è¿½é—®å…·ä½“çš„é”™è¯¯ç æˆ–æ•…éšœç°è±¡ï¼Œä¸è¦ççŒœã€‚
    7.ã€æ’ç‰ˆä¸¥æ ¼è¦æ±‚ã€‘ï¼š
       - å·¥å…·ä¼šè¿”å›å¤šæ®µæ–‡æœ¬ï¼Œæ¯æ®µæ–‡æœ¬åé¢å¯èƒ½ä¼šé™„å¸¦â€œ[è¯¥æ®µè½å…³è”çš„å›¾ç‰‡]â€ã€‚
       - è¯·åœ¨å›ç­”æ—¶ï¼Œå°†å›¾ç‰‡**ç©¿æ’**åœ¨å¯¹åº”çš„æ–‡å­—æè¿°ä¹‹åã€‚
       - ä¾‹å¦‚ï¼šå…ˆè§£é‡Šâ€œç¬¬ä¸€æ­¥ï¼šæŒ‰ä¸‹å¤ä½æŒ‰é’®â€ï¼Œç´§æ¥ç€ç«‹åˆ»å±•ç¤ºå¤ä½æŒ‰é’®çš„å›¾ç‰‡ï¼Œç„¶åå†è¯´â€œç¬¬äºŒæ­¥...â€ã€‚
       - **ç»å¯¹ä¸è¦**æŠŠæ‰€æœ‰å›¾ç‰‡éƒ½å †åœ¨å›ç­”çš„æœ€åé¢ã€‚
       - å›¾ç‰‡é“¾æ¥å¿…é¡»ä¿æŒåŸæ · Markdown æ ¼å¼ `![](http://...)`ã€‚
    """)

# åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™é‡Œå¯ä»¥æ¢æˆ PostgresSaver æˆ– SqliteSaver æ¥æŒä¹…åŒ–å­˜å…¥ç¡¬ç›˜
memory = MemorySaver()

graph = create_react_agent(
    model=llm, 
    tools=tools, 
    prompt=system_prompt,
    checkpointer=memory
)

print("ğŸ¤– å·¥å‚æ™ºèƒ½Agentå·²å¯åŠ¨ï¼")

# å°è£…ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨å‡½æ•°ï¼Œç”¨äºæµå¼è¾“å‡º
async def chat_stream(message: str, thread_id: str):
    config = {"configurable": {"thread_id": thread_id}}
    
    # ä½¿ç”¨ LangGraph çš„ astream æ–¹æ³•
    # æ³¨æ„ï¼šè¿™é‡Œæ ¹æ®ä½  LangGraph çš„å…·ä½“ç‰ˆæœ¬ï¼ŒAPI å¯èƒ½æ˜¯ .stream æˆ– .astream
    async for event in graph.astream_events(
        {"messages": [("user", message)]}, 
        config=config,
        version="v1"
    ):
        # è¿‡æ»¤åªè¿”å› LLM ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡ä¸­é—´æ­¥éª¤çš„æ—¥å¿—
        if event["event"] == "on_chat_model_stream":
             content = event["data"]["chunk"].content
             if content:
                 yield content

# ==============================================================================
# 4. äº¤äº’å¼è¿è¡Œ
# ==============================================================================
def main():
    print("\nä½ å¯ä»¥å¼€å§‹æé—®äº† (è¾“å…¥ 'q' é€€å‡º)")
    
    # å®šä¹‰çº¿ç¨‹ IDï¼ŒLangGraph é€šè¿‡è¿™ä¸ª ID æ¥åŒºåˆ†ä¸åŒçš„å¯¹è¯å†å²
    # å¦‚æœä½ æƒ³å¼€å¯ä¸€æ®µå…¨æ–°çš„å¯¹è¯ï¼ˆå¿˜è®°è¿‡å»ï¼‰ï¼Œåªéœ€è¦æ¢ä¸€ä¸ª ID (ä¾‹å¦‚ "thread_2")
    config = {"configurable": {"thread_id": "factory_user_001"}}
    
    while True:
        user_input = input("\nè¯·æé—®: ")
        if user_input.lower() == 'q':
            break
            
        print("\n[Agent æ€è€ƒä¸­...]")

        # æˆ‘ä»¬åªæŠŠå½“å‰æœ€æ–°çš„è¿™ä¸€å¥è¯ä¼ ç»™ Agent
        # Agent ä¼šæ ¹æ® config é‡Œçš„ thread_id è‡ªåŠ¨å» memory é‡ŒæŸ¥æ‰¾ä¹‹å‰çš„èŠå¤©è®°å½•
        inputs = {"messages": [("user", user_input)]}
        
        # stream_mode="values" ä¼šè¿”å›å½“å‰æ—¶åˆ»å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«å†å²ï¼‰
        # æˆ‘ä»¬åªæ‰“å°æœ€åä¸€æ¡æ–°å¢çš„æ¶ˆæ¯
        for event in graph.stream(inputs, config=config, stream_mode="values"):
            last_message = event["messages"][-1]
            
            # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šåªæ‰“å° AI æ–°ç”Ÿæˆçš„å›å¤
            if last_message.type == "ai" and last_message.content:
                print(f"\n[åŠ©æ‰‹å›ç­”]: {last_message.content}")

if __name__ == "__main__":
    main()