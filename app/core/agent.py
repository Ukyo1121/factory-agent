import os
import sys
import nest_asyncio
nest_asyncio.apply()

import warnings
import logging
import asyncio
import torch
import re
import json
import datetime
import base64
import uuid

warnings.filterwarnings("ignore")
os.environ["TRANSFORMERS_VERBOSITY"] = "error"  # åªæ˜¾ç¤ºä¸¥é‡é”™è¯¯
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1" # å±è”½ Windows ä¸‹çš„ç¬¦å·é“¾æ¥è­¦å‘Š
logging.getLogger("langchain").setLevel(logging.ERROR)
logging.getLogger("langgraph").setLevel(logging.ERROR)

from typing import Annotated, Literal, TypedDict

# --- LlamaIndex ä¾èµ– (ç”¨äº RAG) ---
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.elasticsearch import ElasticsearchStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker
from llama_index.llms.openai_like import OpenAILike

# --- LangGraph & LangChain ä¾èµ– (ç”¨äº Agent) ---
from langgraph.graph import StateGraph
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, ToolMessage, AIMessage
from langgraph.checkpoint.memory import MemorySaver
from typing import TypedDict, Annotated, Sequence
from langgraph.graph import add_messages
from langgraph.prebuilt import ToolNode

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv(override=True)
# å®šä¹‰å¾…è§£ç­”é—®é¢˜çš„æ–‡ä»¶è·¯å¾„
UNANSWERED_FILE = "unanswered_questions.json"
# å®šä¹‰æœ¬åœ°å›¾ç‰‡å­˜å‚¨è·¯å¾„
IMAGES_DIR = "./factory_images"

es_url = os.getenv("ELASTICSEARCH_URL", "http://elasticsearch:9200") 

# ==============================================================================
# 1. å‡†å¤‡ RAG å¼•æ“
# ==============================================================================

# é…ç½® Embedding
GLOBAL_EMBED_MODEL = HuggingFaceEmbedding(model_name="BAAI/bge-m3")

Settings.embed_model = GLOBAL_EMBED_MODEL

Settings.llm = None

# é…ç½® Reranker (æ ¸å¿ƒç«äº‰åŠ›: é‡æ’åº)
reranker = FlagEmbeddingReranker(
    model="BAAI/bge-reranker-base", 
    top_n=5,
    use_fp16=True  # å¿…é¡»å¼€å¯åŠç²¾åº¦ï¼Œè¿›ä¸€æ­¥çœæ˜¾å­˜
)

# ==============================================================================
# 2. å®šä¹‰ Agent çš„å·¥å…· (Tool)
# ==============================================================================

@tool
def search_factory_knowledge(query: str) -> str:
    """
    å½“ç”¨æˆ·è¯¢é—®å·¥å‚è®¾å¤‡æ•…éšœã€é”™è¯¯ç ã€ç»´ä¿®æ­¥éª¤æˆ–æ“ä½œè§„ç¨‹æ—¶ï¼Œå¿…é¡»è°ƒç”¨æ­¤å·¥å…·è¿›è¡ŒæŸ¥è¯¢ã€‚
    é‡è¦æç¤º:query å‚æ•°å¿…é¡»æ˜¯å®Œæ•´çš„ä¸­æ–‡é—®é¢˜å¥å­ï¼Œä¸è¦éšæ„å¯¹ç”¨æˆ·çš„é—®é¢˜è¿›è¡Œæ¦‚æ‹¬ã€ä¸è¦æå–å…³é”®è¯ã€‚
    :param query: å¿…è¦å‚æ•°ï¼Œå­—ç¬¦ä¸²ç±»å‹ï¼Œç”¨äºè¾“å…¥ç”¨æˆ·çš„å…·ä½“é—®é¢˜ã€‚
    :return: è¿”å›æŸ¥è¯¢çš„ç»“æœå’Œæ¥æºæ–‡ä»¶ï¼ŒåŒ…å«å›¾æ–‡æ··æ’å†…å®¹ã€‚
    """
    print(f"\nğŸ” [Agent åŠ¨ä½œ] æ­£åœ¨è°ƒç”¨çŸ¥è¯†åº“æŸ¥è¯¢: {query}")
    vector_store = None
    try:
        # è¿æ¥ ES æ•°æ®åº“
        vector_store = ElasticsearchStore(
            es_url=es_url,
            index_name="factory_knowledge",
        )
        index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

        # RAG Engine
        rag_engine = index.as_query_engine(
            similarity_top_k=10,  # ç²—æ’
            node_postprocessors=[reranker], # ç²¾æ’
            verbose=True,
            response_mode="no_text"
        )
        # è°ƒç”¨ LlamaIndex çš„ RAG å¼•æ“
        response = rag_engine.query(query)

        # ---------------------------------------------------------
        # 1. æ’åºï¼šå…ˆæŒ‰æ–‡ä»¶åï¼Œå†æŒ‰é¡µç 
        # ---------------------------------------------------------
        node_data = []
        if hasattr(response, 'source_nodes'):
            for node in response.source_nodes:
                page_str = node.metadata.get('page_label', '0')
                try:
                    page_num = int(page_str)
                except ValueError:
                    page_num = 0
                
                node_data.append({
                    "text": node.text,
                    "file_name": node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶'),
                    "page_label": page_num
                })

        sorted_nodes = sorted(node_data, key=lambda x: (x['file_name'], x['page_label']))

        # ---------------------------------------------------------
        # 2. æ‹¼æ¥ï¼šæ„å»ºè¿ç»­çš„ä¸Šä¸‹æ–‡æµ
        # ---------------------------------------------------------
        final_context_list = []
        current_file = None
        
        for item in sorted_nodes:
            # å¦‚æœæ¢æ–‡ä»¶äº†ï¼ŒåŠ ä¸€ä¸ªæ˜æ˜¾çš„å¤§æ ‡é¢˜
            if item['file_name'] != current_file:
                final_context_list.append(f"\n\n====== æ–‡ä»¶: {item['file_name']} (å¼€å§‹) ======\n")
                current_file = item['file_name']
            
            # ä½¿ç”¨æ›´ç´§å‡‘çš„åˆ†é¡µæ ‡è®°ï¼Œå¹¶åœ¨æ ‡è®°ä¸­æç¤º LLM æ³¨æ„è·¨é¡µè¿æ¥
            # æˆ‘ä»¬æ•…æ„åœ¨åˆ†é¡µç¬¦å‰åå°‘åŠ æ¢è¡Œï¼Œè®© LLM æ„Ÿè§‰è¿™æ˜¯ä¸€ç¯‡è¿ç»­çš„æ–‡ç« 
            context_str = f"\n{item['text']}"
            final_context_list.append(context_str)

        final_response = "".join(final_context_list) # ä½¿ç”¨ç©ºå­—ç¬¦ä¸²è¿æ¥ï¼Œæ›´ç´§å‡‘
        
        if not final_response.strip():
            return "æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"

        # Debug
        print("âœ… [Debug] å·²æŒ‰é¡µç é‡æ’æ£€ç´¢ç»“æœ")
        print("å†…å®¹é¢„è§ˆï¼š", final_response) # è°ƒè¯•æ—¶å¯å¼€å¯
        
        return final_response
    except Exception as e:
        print(f"âŒ è¯¦ç»†é”™è¯¯: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return f"æŸ¥è¯¢å‡ºé”™: {e}"
    
    finally:
        # æ˜¾å¼å…³é—­ Elasticsearch å®¢æˆ·ç«¯è¿æ¥
        if vector_store is not None:
            try:
                # å…³é—­ ES å®¢æˆ·ç«¯
                if hasattr(vector_store, 'client'):
                    asyncio.get_event_loop().run_until_complete(
                        vector_store.client.close()
                    )
            except Exception as e:
                pass  # å¿½ç•¥å…³é—­æ—¶çš„é”™è¯¯

@tool
def record_missing_knowledge(user_query: str, reason: str = "æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£") -> str:
    """
    å½“ 'search_factory_knowledge' å·¥å…·æ— æ³•åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œæˆ–è€…æ£€ç´¢åˆ°çš„å†…å®¹ä¸ç”¨æˆ·é—®é¢˜ä¸åŒ¹é…æ—¶ï¼Œ
    **å¿…é¡»**è°ƒç”¨æ­¤å·¥å…·å°†é—®é¢˜è®°å½•åˆ°å¾…è§£ç­”åº“ä¸­ã€‚
    :param user_query: ç”¨æˆ·çš„åŸå§‹é—®é¢˜ã€‚
    :param reason: è®°å½•åŸå› ï¼ˆä¾‹å¦‚ï¼šçŸ¥è¯†åº“æ— ç»“æœã€ç»“æœä¸ç›¸å…³ï¼‰ã€‚
    :return: è¿”å›è®°å½•æˆåŠŸçš„æç¤ºã€‚
    """
    print(f"\nğŸ“ [Agent åŠ¨ä½œ] æ­£åœ¨è®°å½•ç¼ºå¤±çŸ¥è¯†: {user_query}")

    # è¯»å–æ—§æ•°æ®
    data = []
    if os.path.exists(UNANSWERED_FILE):
        try:
            with open(UNANSWERED_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
        except:
            data = []
    
    # å»é‡é€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„ query
    for item in data:
        # ä½¿ç”¨ strip() å»é™¤é¦–å°¾ç©ºæ ¼ï¼Œç¡®ä¿åŒ¹é…å‡†ç¡®
        if item.get("query", "").strip() == user_query.strip():
            print(f"âš ï¸ [Agent åŠ¨ä½œ] å‘ç°å¾…è§£ç­”åº“ä¸­å·²å­˜åœ¨è¯¥é—®é¢˜ï¼Œè·³è¿‡å†™å…¥: {user_query}")
            return "è¯¥é—®é¢˜å·²æˆåŠŸè®°å½•åˆ°å¾…è§£ç­”é—®é¢˜åº“ï¼Œè¯·å‘ŠçŸ¥ç”¨æˆ·å·¥ç¨‹å¸ˆå°†åç»­è¡¥å……æ­¤çŸ¥è¯†ã€‚"
        
    # æ„é€ è®°å½•æ•°æ®
    record = {
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "query": user_query,
        "reason": reason,
        "status": "pending" # pending=å¾…äººå·¥å¤„ç†, solved=å·²å…¥åº“
    }

    # è¯»å–æ—§æ•°æ®å¹¶è¿½åŠ 
    data = []
    if os.path.exists(UNANSWERED_FILE):
        try:
            with open(UNANSWERED_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
        except:
            data = []
    
    data.append(record)

    # å†™å…¥æ–‡ä»¶
    with open(UNANSWERED_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return "è¯¥é—®é¢˜å·²æˆåŠŸè®°å½•åˆ°å¾…è§£ç­”é—®é¢˜åº“ï¼Œè¯·å‘ŠçŸ¥ç”¨æˆ·å·¥ç¨‹å¸ˆå°†åç»­è¡¥å……æ­¤çŸ¥è¯†ã€‚"

# å·¥å…·åˆ—è¡¨
tools = [search_factory_knowledge, record_missing_knowledge]

# ==============================================================================
# å¤šæ¨¡æ€å¤„ç†æ ¸å¿ƒå‡½æ•°
# ==============================================================================
def convert_to_multimodal_messages(messages):
    """
    è¿™æ˜¯ä¸€ä¸ªä¸­é—´ä»¶å‡½æ•°ã€‚
    å®ƒçš„ä½œç”¨æ˜¯ï¼šæ£€æŸ¥æœ€è¿‘ä¸€æ¡æ¶ˆæ¯ï¼ˆé€šå¸¸æ˜¯ ToolMessageï¼‰ï¼Œ
    å¦‚æœæœ‰ Markdown å›¾ç‰‡é“¾æ¥ï¼Œå°±æŠŠå®ƒå˜æˆ Base64 å‘ç»™å¤§æ¨¡å‹ã€‚
    """
    processed_messages = list(messages)
    last_msg = processed_messages[-1]

    if isinstance(last_msg, ToolMessage) and "![ç¤ºæ„å›¾]" in str(last_msg.content):
        text_content = last_msg.content
        new_content_blocks = []
        
        # æ­£åˆ™åŒ¹é… Markdown å›¾ç‰‡é“¾æ¥
        pattern = r'!\[.*?\]\((http://localhost:8000/images/(.*?))\)'
        
        last_end = 0
        for match in re.finditer(pattern, text_content):
            start, end = match.span()
            
            # æ·»åŠ å›¾ç‰‡å‰çš„æ–‡å­—
            if start > last_end:
                text_part = text_content[last_end:start]
                if text_part:
                    new_content_blocks.append({"type": "text", "text": text_part})
            
            img_url = match.group(1)
            filename = match.group(2)
            local_path = os.path.join(IMAGES_DIR, filename)
            
            if os.path.exists(local_path):
                try:
                    # [è¿‡æ»¤] å¿½ç•¥å°äº 1.5KB çš„å›¾æ ‡/å™ªç‚¹
                    if os.path.getsize(local_path) < 1500:
                        print(f"âš ï¸ [ä¸­é—´ä»¶] å¿½ç•¥å¾®å‹å›¾ç‰‡: {filename}")
                    else:
                        with open(local_path, "rb") as f:
                            b64_data = base64.b64encode(f.read()).decode("utf-8")
                        
                        new_content_blocks.append({
                            "type": "text", 
                            "text": f"\n[ç³»ç»Ÿæç¤ºï¼šå›¾ç‰‡å¼•ç”¨é“¾æ¥ {img_url}]\n"
                        })
                        new_content_blocks.append({
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{b64_data}"}
                        })
                except Exception as e:
                    print(f"âŒ å›¾ç‰‡å¤„ç†å¼‚å¸¸: {e}")
            
            last_end = end
            
        # æ·»åŠ å‰©ä½™æ–‡å­—
        if last_end < len(text_content):
            tail = text_content[last_end:]
            if tail:
                new_content_blocks.append({"type": "text", "text": tail})
                
        last_msg.content = new_content_blocks
        
    return processed_messages

# ==============================================================================
# 3. æ„å»ºAgent
# ==============================================================================
llm = ChatOpenAI(
    model="qwen3-vl-plus", 
    openai_api_key=os.getenv('DASHSCOPE_API_KEY'),
    openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    temperature=0.1,
    max_tokens=2048,
    model_kwargs={"stream": True} 
)

system_prompt = SystemMessage(content="""
    ### è§’è‰²å®šä¹‰
    ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨ä¸“ä¸šçš„å·¥å‚æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®çŸ¥è¯†åº“çš„å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„æ•…éšœå¤„ç†æˆ–æ“ä½œé—®é¢˜ã€‚
    **ä½ æ‹¥æœ‰è§†è§‰èƒ½åŠ›**ï¼Œå¯ä»¥é˜…è¯»æŸ¥è¯¢ç»“æœä¸­çš„å›¾ç‰‡å†…å®¹ã€‚

    ### åæ­»å¾ªç¯åè®® (æœ€é«˜ä¼˜å…ˆçº§)
    1. **å•æ¬¡æœç´¢åŸåˆ™**ï¼šé’ˆå¯¹ç”¨æˆ·çš„åŒä¸€ä¸ªé—®é¢˜ï¼Œä½ **æœ€å¤šåªèƒ½è°ƒç”¨ä¸‰æ¬¡** `search_factory_knowledge` å·¥å…·ã€‚
    2. **ç¦æ­¢é‡è¯•**ï¼šå¦‚æœä¸‰æ¬¡æœç´¢è¿”å›çš„å†…å®¹éƒ½æ— æ³•æ”¯æ’‘ä½ å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œ**ä¸¥ç¦**å†æ¬¡è°ƒç”¨æœç´¢å·¥å…·ã€‚
    3. **ç«‹å³è®°å½•**ï¼šå‘ç°æ£€ç´¢å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œ**å¿…é¡»ç«‹å³**è°ƒç”¨ `record_missing_knowledge`ï¼Œç»å¯¹ä¸è¦çŠ¹è±«æˆ–å°è¯•è‡ªæˆ‘çº æ­£ã€‚
                              
    ### æ ¸å¿ƒå·¥ä½œæµ (å¿…é¡»ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹æ­¥éª¤)
    **ç¬¬1æ­¥ï¼šæå–å…³é”®å®ä½“**
    - åˆ†æç”¨æˆ·é—®é¢˜ï¼Œæå–æ ¸å¿ƒè®¾å¤‡/ç³»ç»Ÿåç§°ï¼ˆä¾‹å¦‚ï¼šâ€œè‡ªåŠ¨åˆ†æ‹£ç³»ç»Ÿâ€ã€â€œFANUCæœºå™¨äººâ€ã€â€œä¼ é€å¸¦â€ï¼‰ã€‚
    - è®°ä½è¿™ä¸ªæ ¸å¿ƒå®ä½“ï¼Œå®ƒæ˜¯æœ¬æ¬¡å›ç­”çš„â€œä¸»è¯­â€ã€‚

    **ç¬¬2æ­¥ï¼šæŸ¥è¯¢å¹¶å®¡æŸ¥ (å…³é”®ä¸€æ­¥)**
    - è°ƒç”¨ `search_factory_knowledge` æŸ¥è¯¢çŸ¥è¯†åº“ã€‚
    - **å®¡æŸ¥æŸ¥è¯¢ç»“æœçš„ä¸»è¯­**ï¼š
      - ä»”ç»†é˜…è¯»æŸ¥è¯¢åˆ°çš„æ¯ä¸€æ®µæ–‡å­—ï¼Œå¯»æ‰¾å…¶ä¸­æåˆ°çš„è®¾å¤‡åç§°ã€‚
      - **åŒ¹é…æ£€æŸ¥ç¤ºä¾‹**ï¼š
        - ç”¨æˆ·é—®ï¼šâ€œè‡ªåŠ¨åˆ†æ‹£ç³»ç»Ÿâ€ -> æŸ¥è¯¢å†…å®¹ï¼šâ€œæœºå™¨äººæ‰‹åŠ¨æ“ä½œ...â€ -> **ä¸åŒ¹é…ï¼** (è¿™æ˜¯å¼ å† ææˆ´)
        - ç”¨æˆ·é—®ï¼šâ€œè‡ªåŠ¨åˆ†æ‹£ç³»ç»Ÿâ€ -> æŸ¥è¯¢å†…å®¹ï¼šâ€œåˆ†æ‹£å•å…ƒæ“ä½œ...â€ -> **åŒ¹é…ã€‚**
        - ç”¨æˆ·é—®ï¼šâ€œè‡ªåŠ¨åˆ†æ‹£ç³»ç»Ÿâ€ -> æŸ¥è¯¢å†…å®¹å®Œå…¨æ²¡æè®¾å¤‡åï¼Œåªè¯´â€œæŒ‰ä¸‹çº¢è‰²æŒ‰é’®â€ -> **é«˜é£é™©ï¼** é™¤éä½ èƒ½ä»ä¸Šä¸‹æ–‡ï¼ˆå¦‚æ–‡ä»¶åï¼‰ç¡®ä¿¡è¿™æ˜¯åˆ†æ‹£ç³»ç»Ÿï¼Œå¦åˆ™è§†ä¸ºä¸åŒ¹é…ã€‚
    - **å®¡æŸ¥å›¾ç‰‡å†…å®¹ (è§†è§‰èƒ½åŠ›)**ï¼š
      - ä½ ä¼šçœ‹åˆ°ç©¿æ’åœ¨æ–‡å­—ä¸­çš„å›¾ç‰‡ã€‚
      - **è¯·ä»”ç»†çœ‹å›¾**ï¼šåˆ¤æ–­å›¾ç‰‡å†…å®¹æ˜¯â€œè®¾å¤‡æ“ä½œç¤ºæ„å›¾/ç”µè·¯å›¾/å®ç‰©å›¾â€è¿˜æ˜¯â€œæ— æ„ä¹‰çš„Logo/é¡µçœ‰â€ã€‚
      - **å†³ç­–**ï¼šåªæœ‰å½“å›¾ç‰‡èƒ½è¾…åŠ©è¯´æ˜æ“ä½œæ­¥éª¤æ—¶ï¼Œæ‰ä¿ç•™å®ƒï¼›å¦‚æœæ˜¯æ— å…³å›¾ç‰‡ï¼Œè¯·ç›´æ¥å¿½ç•¥ï¼Œä¸è¦è¾“å‡ºã€‚
                              
    **ç¬¬3æ­¥ï¼šå†³ç­–ä¸è¡ŒåŠ¨**
    - **æƒ…å†µ A (ä¸»è¯­åŒ¹é… ä¸” å†…å®¹ç›¸å…³)**ï¼š
      - å¯¹æŸ¥è¯¢çš„ç»“æœè¿›è¡Œæ•´åˆæˆ–æå–ï¼Œæ¸…æ™°å‡†ç¡®åœ°å›ç­”ç”¨æˆ·ã€‚
      - **å›¾æ–‡æ··æ’è§„åˆ™**ï¼š
        - ä½ çš„å›ç­”å¿…é¡»å›¾æ–‡å¹¶èŒ‚ã€‚
        - å¼•ç”¨å›¾ç‰‡æ—¶ï¼Œè¯·ä½¿ç”¨ Markdown æ ¼å¼ï¼š`![ç¤ºæ„å›¾](å›¾ç‰‡é“¾æ¥)`ã€‚
        - **æ³¨æ„**ï¼šåªèƒ½ä½¿ç”¨ç³»ç»Ÿæç¤ºä¸­ç»™å‡ºçš„ `http://localhost...` é“¾æ¥ï¼Œ**ç»å¯¹ä¸è¦**è¾“å‡º Base64 ç¼–ç ã€‚
    - **æƒ…å†µ B (ä¸»è¯­ä¸åŒ¹é… æˆ– æŸ¥è¯¢å·¥å…·è¿”å›â€œæœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹â€ æˆ– è¿”å›çš„å†…å®¹ä¸ç”¨æˆ·é—®é¢˜çš„å…³è”æ€§å¾ˆä½ï¼Œä¸è¶³ä»¥æ”¯æ’‘ä½ å›ç­”ç”¨æˆ·çš„é—®é¢˜)**ï¼š
      - **ç»å¯¹ç¦æ­¢**å¼ºè¡Œæ‹¼å‡‘ç­”æ¡ˆã€‚ä¾‹å¦‚ï¼šä¸è¦æŠŠæœºå™¨äººçš„æ“ä½œå®‰åœ¨åˆ†æ‹£ç³»ç»Ÿå¤´ä¸Šã€‚
      - **å¿…é¡»**è°ƒç”¨ `record_missing_knowledge` å·¥å…·ï¼Œå°†é—®é¢˜è®°å½•åˆ°å¾…è§£ç­”åº“ã€‚
      - ç¤¼è²Œå›å¤ç”¨æˆ·ï¼šâ€œæŠ±æ­‰ï¼Œå½“å‰çŸ¥è¯†åº“ä¸­æš‚æœªæ”¶å½•æ­¤é—®é¢˜ã€‚ä½†æˆ‘å·²å°†å…¶è‡ªåŠ¨è®°å½•åˆ°ã€å¾…è§£ç­”é—®é¢˜åº“ã€‘ï¼Œå·¥ç¨‹å¸ˆå°†åœ¨åç»­æ›´æ–°ä¸­è¡¥å……è¯¥å†…å®¹ã€‚â€
                              
    ### æ³¨æ„äº‹é¡¹
    1. **å®Œæ•´æ€§**ï¼šä½ è¾“å‡ºçš„å†…å®¹åŠ¡å¿…èƒ½å¤Ÿ**å®Œæ•´**åœ°å¥‘åˆç”¨æˆ·çš„é—®é¢˜ï¼Œä¾‹å¦‚ç”¨æˆ·æé—®â€œè‡ªåŠ¨åˆ†æ‹£ç³»ç»Ÿçš„æ‰‹åŠ¨æ“ä½œæµç¨‹â€ï¼ŒæŸ¥è¯¢å·¥å…·è¿”å›çš„å†…å®¹åªåŒ…å«â€œæ‰‹åŠ¨æ“ä½œæµç¨‹â€ï¼Œä½†ç¼ºå°‘â€œè‡ªåŠ¨åˆ†æ‹£ç³»ç»Ÿâ€è¿™ä¸ªå…³é”®è¯ï¼Œä¹Ÿè¦è§†ä¸ºæ— æ³•å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œéœ€è¦å°†è¯¥é—®é¢˜å­˜å…¥å¾…è§£ç­”é—®é¢˜åº“ã€‚
    2. **å›¾æ–‡å¯¹åº”**ï¼šå¦‚æœæŸ¥è¯¢å·¥å…·è¿”å›å†…å®¹ä¸­çš„æŸä¸€æ­¥éª¤æœ‰å›¾ï¼Œä½ åœ¨å›ç­”è¯¥æ­¥éª¤æ—¶å°±å¿…é¡»å¸¦ä¸Šé‚£å¼ å›¾ã€‚ä¸è¦é—æ¼ã€‚å›¾ç‰‡åº”è¯¥ç´§è·Ÿåœ¨å®ƒæ‰€è§£é‡Šçš„æ­¥éª¤æ–‡å­—ä¹‹åã€‚
    3. **ä¸¥ç¦ç¼–é€ **ï¼šä¸å…è®¸åœ¨æŸ¥è¯¢å·¥å…·è¿”å›çš„å†…å®¹ä¸Šå¢åŠ æ— ä¸­ç”Ÿæœ‰çš„å†…å®¹ï¼Œä½ åªèƒ½å¯¹æŸ¥è¯¢çš„ç»“æœè¿›è¡Œæ•´åˆæˆ–æå–ï¼Œç„¶åæ¸…æ™°åœ°å›ç­”ç”¨æˆ·ï¼Œ**ä¸¥ç¦ç¼–é€ **ã€‚
    4. **ç¡®å®šæ€§**ï¼šå¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸æ¸…æ™°ï¼ˆä¾‹å¦‚åªè¯´äº†â€œæœºå™¨åäº†â€ï¼‰ï¼Œè¯·è¿½é—®å…·ä½“çš„é”™è¯¯ç æˆ–æ•…éšœç°è±¡ç­‰é—®é¢˜çš„ç»†èŠ‚ï¼Œä¸è¦ççŒœã€‚
                              
    ### å·¥å…·è°ƒç”¨æ ¼å¼è§„èŒƒ
    **ä½ å¿…é¡»ä½¿ç”¨æ ‡å‡†çš„ OpenAI Function Calling æ ¼å¼ã€‚**
    **ä¸¥ç¦**è¾“å‡º `<tool_call>`, `<function>` ç­‰ XML æ ‡ç­¾ã€‚
    **ä¸¥ç¦**è¾“å‡º Base64 ç¼–ç ã€‚
                              
    ### å›ç­”æ ¼å¼
    - ä½¿ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼ã€‚
    - åœ¨å›ç­”æœ«å°¾åˆ—å‡ºã€å‚è€ƒæ¥æºæ–‡ä»¶ã€‘ã€‚
    """)

# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]

# å®šä¹‰èŠ‚ç‚¹ï¼šè°ƒç”¨æ¨¡å‹
async def call_model(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]

    # 1. [è®°å½•å·¥å…·é˜²æ­»å¾ªç¯æ‹¦æˆª] ä¿æŒä¸å˜
    # å¦‚æœåˆšæ‰§è¡Œå®Œè®°å½•å·¥å…·ï¼Œç›´æ¥ç»“æŸï¼Œä¸è®©æ¨¡å‹å†åºŸè¯
    if isinstance(last_message, ToolMessage) and "è¯¥é—®é¢˜å·²æˆåŠŸè®°å½•åˆ°å¾…è§£ç­”é—®é¢˜åº“" in str(last_message.content):
        print("ğŸ›‘ [ç³»ç»Ÿæ‹¦æˆª] æ£€æµ‹åˆ°åˆšåˆšæ‰§è¡Œäº†è®°å½•å·¥å…·ï¼Œå¼ºåˆ¶ç»“æŸå¯¹è¯å¾ªç¯ã€‚")
        return {
            "messages": [
                AIMessage(content="æŠ±æ­‰ï¼Œå½“å‰çŸ¥è¯†åº“ä¸­æš‚æœªæ”¶å½•æ­¤é—®é¢˜ã€‚æˆ‘å·²å°†å…¶è‡ªåŠ¨è®°å½•åˆ°ã€å¾…è§£ç­”é—®é¢˜åº“ã€‘ï¼Œå·¥ç¨‹å¸ˆå°†åœ¨åç»­æ›´æ–°ä¸­è¡¥å……è¯¥å†…å®¹ã€‚")
            ]
        }

    print("ğŸ¤– [Agent åŠ¨ä½œ] æ­£åœ¨æ€è€ƒ (è°ƒç”¨å¤§æ¨¡å‹)...")
    
    # 2. ç¡®ä¿ SystemPrompt åœ¨æœ€å‰
    if not isinstance(messages[0], SystemMessage):
        messages = [system_prompt] + messages
    else:
        messages[0] = system_prompt

    # 3. æ‰§è¡Œä¸­é—´ä»¶ï¼šå¤„ç†å›¾ç‰‡ Base64
    messages_with_images = convert_to_multimodal_messages(messages)
    
    # ==================== [æ™ºèƒ½æ£€æµ‹æ˜¯å¦æœè¿‡] ====================
    has_searched = False
    
    # å€’åºéå†æ¶ˆæ¯ï¼Œåªæ£€æŸ¥â€œå½“å‰ç”¨æˆ·æé—®ä¹‹åâ€äº§ç”Ÿçš„åŠ¨ä½œ
    for i in range(len(messages) - 1, -1, -1):
        msg = messages[i]
        
        # é‡åˆ°ç”¨æˆ·æ¶ˆæ¯ï¼Œè¯´æ˜å›åˆ°äº†ä¸Šä¸€è½®ï¼Œåœæ­¢æ£€æŸ¥
        if isinstance(msg, HumanMessage):
            break
            
        # æ£€æŸ¥ AI æ¶ˆæ¯ä¸­çš„å·¥å…·è°ƒç”¨
        if isinstance(msg, AIMessage) and msg.tool_calls:
            for tc in msg.tool_calls:
                if tc["name"] == "search_factory_knowledge":
                    has_searched = True
                    break
        
        if has_searched:
            break
    
    # åŠ¨æ€æ„å»ºå·¥å…·åˆ—è¡¨
    current_tools = list(tools)
    if has_searched:
        print("ğŸ›‘ [ç³»ç»Ÿå¼ºåˆ¶] æ£€æµ‹åˆ°**æœ¬è½®å¯¹è¯**å·²æ‰§è¡Œè¿‡æœç´¢ï¼Œæ­£åœ¨ç§»é™¤æœç´¢å·¥å…·...")
        current_tools = [t for t in tools if t.name != "search_factory_knowledge"]
    
    model_with_tools = llm.bind_tools(current_tools)
    # ====================================================================
    
    try:
        response = await model_with_tools.ainvoke(messages_with_images)
        
        # ==================== [XML å¼ºåŠ›ä¿®å¤è¡¥ä¸] ====================
        content_str = str(response.content)
        
        if not response.tool_calls and ("<tool_call>" in content_str or "<function=" in content_str):
            print(f"âš ï¸ [å…¼å®¹æ€§ä¿®å¤] æ£€æµ‹åˆ° Qwen è¿”å›äº† XML...")
            
            func_pattern = r"<function=['\"]?(\w+)['\"]?>| <function=['\"]?(\w+)['\"]?>"
            func_match = re.search(func_pattern, content_str)
            
            if func_match:
                func_name = func_match.group(1) or func_match.group(2)
                
                # --- [é˜²æ­»å¾ªç¯æ‹¦æˆªå™¨] ---
                # å¦‚æœæœ¬è½®æœè¿‡äº†ï¼Œä½†æ¨¡å‹è¿˜æƒ³æœï¼Œå¼ºåˆ¶è½¬ä¸ºè®°å½•
                if has_searched and func_name == "search_factory_knowledge":
                    print("ğŸ›¡ï¸ [æ‹¦æˆªæˆåŠŸ] æ¨¡å‹è¯•å›¾äºŒæ¬¡æœç´¢ï¼Œç³»ç»Ÿå¼ºåˆ¶è½¬æ¢ä¸ºâ€˜è®°å½•ç¼ºå¤±çŸ¥è¯†â€™...")
                    func_name = "record_missing_knowledge"
                    q_match = re.search(r"<parameter=query>(.*?)</parameter>", content_str, re.DOTALL)
                    query_val = q_match.group(1).strip() if q_match else "ç”¨æˆ·é‡åˆ°çš„æœªçŸ¥é—®é¢˜"
                    
                    response.tool_calls = [{
                        "name": func_name,
                        "args": {
                            "user_query": query_val,
                            "reason": "è‡ªåŠ¨æ‹¦æˆªï¼šçŸ¥è¯†åº“å•æ¬¡æ£€ç´¢æ— æœï¼Œå¼ºåˆ¶è½¬å…¥å¾…è§£ç­”åº“"
                        },
                        "id": f"call_{uuid.uuid4().hex[:8]}"
                    }]
                    response.content = ""
                    return {"messages": [response]}
                # -----------------------

                args = {}
                if func_name == "search_factory_knowledge":
                    q_match = re.search(r"<parameter=query>(.*?)</parameter>", content_str, re.DOTALL)
                    if q_match: args["query"] = q_match.group(1).strip()
                        
                elif func_name == "record_missing_knowledge":
                    uq_match = re.search(r"<parameter=user_query>(.*?)</parameter>", content_str, re.DOTALL)
                    if uq_match: args["user_query"] = uq_match.group(1).strip()
                    r_match = re.search(r"<parameter=reason>(.*?)</parameter>", content_str, re.DOTALL)
                    if r_match: args["reason"] = r_match.group(1).strip()
                    else: args["reason"] = "æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£"
                
                if args:
                    print(f"ğŸ”§ [ä¿®å¤æˆåŠŸ] æå–åˆ°å·¥å…·: {func_name}, å‚æ•°: {args}")
                    response.tool_calls = [{
                        "name": func_name,
                        "args": args,
                        "id": f"call_{uuid.uuid4().hex[:8]}"
                    }]
                    response.content = "" 
            
            if "<tool_call>" in str(response.content):
                clean_content = re.sub(r"<tool_call>.*?</tool_call>", "", str(response.content), flags=re.DOTALL)
                response.content = clean_content.strip()

        print("âœ… [Agent åŠ¨ä½œ] å¤§æ¨¡å‹æ€è€ƒå®Œæˆ")
        return {"messages": [response]}
        
    except Exception as e:
        print(f"âŒ [Agent æŠ¥é”™] æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        return {"messages": [AIMessage(content=f"æ¨¡å‹è°ƒç”¨å‡ºé”™: {str(e)}")]}

# å®šä¹‰è¾¹ï¼šåˆ¤æ–­æ˜¯å¦ç»“æŸ
def should_continue(state: AgentState):
    last_message = state["messages"][-1]
    if last_message.tool_calls:
        return "tools"
    return "__end__"

# --- æ„å»ºå›¾ ---
workflow = StateGraph(AgentState)

workflow.add_node("agent", call_model)
workflow.add_node("tools", ToolNode(tools))

workflow.set_entry_point("agent")

workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "__end__": "__end__"
    }
)
workflow.add_edge("tools", "agent")

# ç¼–è¯‘å›¾
memory = MemorySaver()
graph = workflow.compile(checkpointer=memory)

print("ğŸ¤– å·¥å‚æ™ºèƒ½Agentå·²å¯åŠ¨ï¼")

# å°è£…ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨å‡½æ•°ï¼Œç”¨äºæµå¼è¾“å‡º
async def chat_stream(message: str, thread_id: str):
    config = {"configurable": {"thread_id": thread_id}}
    has_yielded = False # æ ‡è®°æ˜¯å¦å·²ç»å‘å‰ç«¯å‘é€è¿‡å†…å®¹
    
    async for event in graph.astream_events(
        {"messages": [HumanMessage(content=message)]}, 
        config=config,
        version="v1"
    ):
        # 1. æ•è·æµå¼ Token (LLM æ­£å¸¸ç”Ÿæˆæ—¶)
        if event["event"] == "on_chat_model_stream":
             content = event["data"]["chunk"].content
             if content:
                 if "<tool_call>" in content or "<function=" in content: continue
                 has_yielded = True
                 yield content
        
        # 2. æ•è·éæµå¼æœ€ç»ˆç»“æœ (LLM ä¸€æ¬¡æ€§ç”Ÿæˆæ—¶)
        elif event["event"] == "on_chat_model_end" and not has_yielded:
            output = event["data"]["output"]
            if hasattr(output, "generations") and output.generations:
                msg = output.generations[0][0].message
                if isinstance(msg, BaseMessage) and msg.type == "ai" and msg.content:
                    if not msg.tool_calls:
                        if "<tool_call>" in msg.content: continue
                        has_yielded = True
                        yield msg.content

        # 3. æ•è·ç³»ç»Ÿæ‹¦æˆª/ç¡¬ç¼–ç æ¶ˆæ¯] 
        # å½“ call_model ç›´æ¥ return AIMessage (è·³è¿‡å¤§æ¨¡å‹) æ—¶ï¼Œè§¦å‘çš„æ˜¯ on_chain_end
        elif event["event"] == "on_chain_end" and event["name"] == "agent":
            # åªæœ‰å½“ä¹‹å‰æ²¡æœ‰ä» LLM æ‹¿åˆ°æ•°æ®æ—¶ï¼Œæ‰æ£€æŸ¥è¿™é‡Œçš„è¾“å‡º
            if not has_yielded:
                outputs = event["data"].get("output")
                if outputs and isinstance(outputs, dict) and "messages" in outputs:
                    last_msg = outputs["messages"][-1]
                    # ç¡®ä¿æ˜¯ AI æ¶ˆæ¯ä¸”æœ‰å†…å®¹
                    if isinstance(last_msg, AIMessage) and last_msg.content:
                        # å†æ¬¡æ£€æŸ¥æ˜¯ä¸æ˜¯ XML ä¹±ç ï¼ˆåŒé‡ä¿é™©ï¼‰
                        if "<tool_call>" in last_msg.content: continue
                        
                        has_yielded = True
                        yield last_msg.content
        # ========================================================================

# ==============================================================================
# 4. äº¤äº’å¼è¿è¡Œ
# ==============================================================================
def main():
    print("\nä½ å¯ä»¥å¼€å§‹æé—®äº† (è¾“å…¥ 'q' é€€å‡º)")
    
    # å®šä¹‰çº¿ç¨‹ IDï¼ŒLangGraph é€šè¿‡è¿™ä¸ª ID æ¥åŒºåˆ†ä¸åŒçš„å¯¹è¯å†å²
    # å¦‚æœä½ æƒ³å¼€å¯ä¸€æ®µå…¨æ–°çš„å¯¹è¯ï¼ˆå¿˜è®°è¿‡å»ï¼‰ï¼Œåªéœ€è¦æ¢ä¸€ä¸ª ID (ä¾‹å¦‚ "thread_2")
    config = {"configurable": {"thread_id": "factory_user_001"}}
    
    while True:
        user_input = input("\nè¯·æé—®: ")
        if user_input.lower() == 'q':
            break
            
        print("\n[Agent æ€è€ƒä¸­...]")

        # æˆ‘ä»¬åªæŠŠå½“å‰æœ€æ–°çš„è¿™ä¸€å¥è¯ä¼ ç»™ Agent
        # Agent ä¼šæ ¹æ® config é‡Œçš„ thread_id è‡ªåŠ¨å» memory é‡ŒæŸ¥æ‰¾ä¹‹å‰çš„èŠå¤©è®°å½•
        inputs = {"messages": [("user", user_input)]}
        
        # stream_mode="values" ä¼šè¿”å›å½“å‰æ—¶åˆ»å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«å†å²ï¼‰
        # æˆ‘ä»¬åªæ‰“å°æœ€åä¸€æ¡æ–°å¢çš„æ¶ˆæ¯
        for event in graph.stream(inputs, config=config, stream_mode="values"):
            last_message = event["messages"][-1]
            
            # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šåªæ‰“å° AI æ–°ç”Ÿæˆçš„å›å¤
            if last_message.type == "ai" and last_message.content:
                print(f"\n[åŠ©æ‰‹å›ç­”]: {last_message.content}")

if __name__ == "__main__":
    main()