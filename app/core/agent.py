import os
import sys
import nest_asyncio
nest_asyncio.apply()

import warnings
import logging
import asyncio
import torch
import re

warnings.filterwarnings("ignore")
os.environ["TRANSFORMERS_VERBOSITY"] = "error"  # åªæ˜¾ç¤ºä¸¥é‡é”™è¯¯
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1" # å±è”½ Windows ä¸‹çš„ç¬¦å·é“¾æ¥è­¦å‘Š
logging.getLogger("langchain").setLevel(logging.ERROR)
logging.getLogger("langgraph").setLevel(logging.ERROR)

from typing import Annotated, Literal, TypedDict

# --- LlamaIndex ä¾èµ– (ç”¨äº RAG) ---
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.elasticsearch import ElasticsearchStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker
from llama_index.llms.openai_like import OpenAILike

# --- LangGraph & LangChain ä¾èµ– (ç”¨äº Agent) ---
from langgraph.graph import StateGraph
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv(override=True)

# ==============================================================================
# 1. å‡†å¤‡ RAG å¼•æ“
# ==============================================================================

# é…ç½® Embedding
GLOBAL_EMBED_MODEL = HuggingFaceEmbedding(model_name="BAAI/bge-m3")

Settings.embed_model = GLOBAL_EMBED_MODEL

# é…ç½®llm
Settings.llm = OpenAILike(
        model="deepseek-chat",
        api_key=os.getenv('DEEPSEEK_API_KEY'),
        api_base="https://api.deepseek.com",
        is_chat_model=True,
        context_window=32768,
        temperature=0.1,
        max_tokens=1024
    )

# é…ç½® Reranker (æ ¸å¿ƒç«äº‰åŠ›: é‡æ’åº)
reranker = FlagEmbeddingReranker(
    model="BAAI/bge-reranker-base", 
    top_n=5,
    use_fp16=True  # å¿…é¡»å¼€å¯åŠç²¾åº¦ï¼Œè¿›ä¸€æ­¥çœæ˜¾å­˜
)

# ==============================================================================
# 2. å®šä¹‰ Agent çš„å·¥å…· (Tool)
# ==============================================================================

@tool
def search_factory_knowledge(query: str) -> str:
    """
    å½“ç”¨æˆ·è¯¢é—®å·¥å‚è®¾å¤‡æ•…éšœã€é”™è¯¯ç ã€ç»´ä¿®æ­¥éª¤æˆ–æ“ä½œè§„ç¨‹æ—¶ï¼Œå¿…é¡»è°ƒç”¨æ­¤å·¥å…·è¿›è¡ŒæŸ¥è¯¢ã€‚
    é‡è¦æç¤º:query å‚æ•°å¿…é¡»æ˜¯å®Œæ•´çš„ä¸­æ–‡é—®é¢˜å¥å­ï¼Œä¸è¦éšæ„å¯¹ç”¨æˆ·çš„é—®é¢˜è¿›è¡Œæ¦‚æ‹¬ã€ä¸è¦æå–å…³é”®è¯ã€‚
    :param query: å¿…è¦å‚æ•°ï¼Œå­—ç¬¦ä¸²ç±»å‹ï¼Œç”¨äºè¾“å…¥ç”¨æˆ·çš„å…·ä½“é—®é¢˜ã€‚
    :return: è¿”å›æŸ¥è¯¢çš„ç»“æœå’Œæ¥æºæ–‡ä»¶
    """
    print(f"\nğŸ” [Agent åŠ¨ä½œ] æ­£åœ¨è°ƒç”¨çŸ¥è¯†åº“æŸ¥è¯¢: {query}")
    vector_store = None
    try:
        # è¿æ¥ ES æ•°æ®åº“
        vector_store = ElasticsearchStore(
            es_url="http://localhost:9200",
            index_name="factory_knowledge",
        )
        index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

        # RAG Engine
        rag_engine = index.as_query_engine(
            similarity_top_k=15,  # ç²—æ’
            node_postprocessors=[reranker], # ç²¾æ’
            verbose=True
        )
        # è°ƒç”¨ LlamaIndex çš„ RAG å¼•æ“
        response = rag_engine.query(query)
        file_names = set() # ä½¿ç”¨é›†åˆå»é‡
        found_images = set() # ä½¿ç”¨é›†åˆå»é‡

        if hasattr(response, 'source_nodes'):
            for node in response.source_nodes:
                # 1. è·å–æ–‡ä»¶å
                fname = node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                file_names.add(fname)
                
                # 2. ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä»å…ƒæ•°æ®ä¸­è·å–å…³è”çš„å›¾ç‰‡åˆ—è¡¨
                # æ³¨æ„ï¼šå› ä¸º metadata ä¼šè·Ÿç€åˆ‡ç‰‡èµ°ï¼Œæ‰€ä»¥æ£€ç´¢åˆ°çš„æ¯ä¸ªåˆ‡ç‰‡é‡Œéƒ½æœ‰å®Œæ•´çš„å›¾ç‰‡åˆ—è¡¨
                # æˆ‘ä»¬éœ€è¦å»é‡ï¼Œå¦åˆ™ 5 ä¸ªåˆ‡ç‰‡ä¼šé‡å¤æ˜¾ç¤º 5 æ¬¡å›¾ç‰‡
                images_in_metadata = node.metadata.get("image_files", [])
                
                # images_in_metadata åº”è¯¥æ˜¯ä¸€ä¸ªæ–‡ä»¶ååˆ—è¡¨ ['xxx_p1.png', 'xxx_p2.png']
                if isinstance(images_in_metadata, list):
                    for img_name in images_in_metadata:
                        found_images.add(img_name)

        # æ„å»ºè¿”å›ç»™ LLM çš„æœ€ç»ˆæ–‡æœ¬
        final_response = str(response)
        
        # è¿½åŠ æ¥æºæ–‡ä»¶
        if file_names:
            final_response += f"\n\nã€å‚è€ƒæ¥æºæ–‡ä»¶ã€‘: {', '.join(file_names)}"
        
        # è¿½åŠ å›¾ç‰‡é“¾æ¥ (æ„å»º Markdown æ ¼å¼)
        if found_images:
            # ä¸ºäº†é¿å…ä¸€æ¬¡æ€§å±•ç¤ºå‡ åå¼ å›¾ï¼Œæˆ‘ä»¬å¯ä»¥åšä¸€ä¸ªç®€å•çš„ç­›é€‰ç­–ç•¥
            # æ¯”å¦‚ï¼šåªæ˜¾ç¤ºå‰ 5 å¼ ï¼Œæˆ–è€…å…¨éƒ¨æ˜¾ç¤º
            # è¿™é‡Œæ¼”ç¤ºå…¨éƒ¨æ˜¾ç¤º
            sorted_images = sorted(list(found_images)) # æ’åºï¼Œä¿è¯é¡ºåº
            
            final_response += "\n\nã€ç›¸å…³å›¾ç‰‡è¯æ®ã€‘:\n"
            for img_name in sorted_images:
                # æ‹¼æ¥å®Œæ•´çš„ URL
                img_url = f"http://localhost:8000/images/{img_name}"
                final_response += f"![å‚è€ƒå›¾]({img_url})\n"

        # -------------------------------------------------------------
        # æ‰“å°è°ƒè¯•ä¿¡æ¯ (ä¸€å®šè¦çœ‹è¿™ä¸ªï¼)
        # -------------------------------------------------------------
        print(f"--- [Debug] Tool Output ---")
        print(f"Found Images: {len(found_images)}")
        # print(final_response[:500]) # æ‰“å°å‰500å­—çœ‹çœ‹
        print("---------------------------")
        
        return final_response
    except Exception as e:
        print(f"âŒ è¯¦ç»†é”™è¯¯: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return f"æŸ¥è¯¢å‡ºé”™: {e}"
    
    finally:
        # æ˜¾å¼å…³é—­ Elasticsearch å®¢æˆ·ç«¯è¿æ¥
        if vector_store is not None:
            try:
                # å…³é—­ ES å®¢æˆ·ç«¯
                if hasattr(vector_store, 'client'):
                    asyncio.get_event_loop().run_until_complete(
                        vector_store.client.close()
                    )
            except Exception as e:
                pass  # å¿½ç•¥å…³é—­æ—¶çš„é”™è¯¯

# å·¥å…·åˆ—è¡¨
tools = [search_factory_knowledge]

# ==============================================================================
# 3. æ„å»ºAgent
# ==============================================================================

llm = ChatOpenAI(
    model="deepseek-chat",
    openai_api_key=os.getenv('DEEPSEEK_API_KEY'),
    openai_api_base="https://api.deepseek.com",
    temperature=0.1
)

system_prompt = SystemMessage(content="""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥å‚æ™ºèƒ½åŠ©æ‰‹ã€‚
    1. é‡åˆ°æ•…éšœé—®é¢˜æˆ–æ“ä½œé—®é¢˜ï¼Œå¿…é¡»ä¼˜å…ˆä½¿ç”¨ 'search_factory_knowledge' å·¥å…·æŸ¥è¯¢çŸ¥è¯†åº“ã€‚
    2. å¦‚æœæŸ¥è¯¢å·¥å…·è¿”å›äº†è§£å†³æ–¹æ³•æˆ–æ“ä½œæ­¥éª¤ï¼Œè¯·æ¸…æ™°åœ°è½¬è¿°ç»™ç”¨æˆ·ï¼Œå¹¶å‘Šè¯‰ç”¨æˆ·å‚è€ƒæ¥æºæ–‡ä»¶çš„åç§°ã€‚
    3. å¦‚æœå·¥å…·è¿”å›çš„å†…å®¹ä¸­åŒ…å«å›¾ç‰‡é“¾æ¥ï¼ˆMarkdownæ ¼å¼å¦‚ ![](http://...)ï¼‰ï¼Œè¯·åŠ¡å¿…åœ¨å›ç­”çš„å¯¹åº”ä½ç½®åŸæ ·å±•ç¤ºè¿™äº›å›¾ç‰‡ï¼Œä¸è¦å¿½ç•¥å®ƒä»¬ï¼Œä¹Ÿä¸è¦ä¿®æ”¹é“¾æ¥åœ°å€ã€‚å›¾ç‰‡å¯¹äºç”¨æˆ·ç†è§£æ“ä½œæ­¥éª¤éå¸¸é‡è¦ã€‚
    4. ä¸å…è®¸åœ¨æŸ¥è¯¢å·¥å…·è¿”å›çš„å†…å®¹ä¸Šå¢åŠ æ— ä¸­ç”Ÿæœ‰çš„å†…å®¹ï¼Œä½ åªèƒ½å¯¹æŸ¥è¯¢çš„ç»“æœè¿›è¡Œæ•´åˆå¹¶æ¸…æ™°åœ°å›ç­”ç”¨æˆ·ã€‚
    5. å¦‚æœæŸ¥è¯¢ç»“æœä¸è¶³ä»¥æ”¯æ’‘ä½ å›ç­”ç”¨æˆ·çš„é—®é¢˜æˆ–ä¸ç”¨æˆ·çš„é—®é¢˜å…³è”æ€§å¾ˆå°ï¼Œè¯·ä½ è€å®å›ç­”æŸ¥è¯¢ä¸åˆ°ç›¸å…³ç»“æœï¼Œæˆ–è¿½é—®ç”¨æˆ·é—®é¢˜çš„ç»†èŠ‚ã€‚
    6. å¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸æ¸…æ™°ï¼ˆä¾‹å¦‚åªè¯´äº†â€œæœºå™¨åäº†â€ï¼‰ï¼Œè¯·è¿½é—®å…·ä½“çš„é”™è¯¯ç æˆ–æ•…éšœç°è±¡ï¼Œä¸è¦ççŒœã€‚
    """)

# åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™é‡Œå¯ä»¥æ¢æˆ PostgresSaver æˆ– SqliteSaver æ¥æŒä¹…åŒ–å­˜å…¥ç¡¬ç›˜
memory = MemorySaver()

graph = create_react_agent(
    model=llm, 
    tools=tools, 
    prompt=system_prompt,
    checkpointer=memory
)

print("ğŸ¤– å·¥å‚æ™ºèƒ½Agentå·²å¯åŠ¨ï¼")

# å°è£…ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨å‡½æ•°ï¼Œç”¨äºæµå¼è¾“å‡º
async def chat_stream(message: str, thread_id: str):
    config = {"configurable": {"thread_id": thread_id}}
    
    # ä½¿ç”¨ LangGraph çš„ astream æ–¹æ³•
    # æ³¨æ„ï¼šè¿™é‡Œæ ¹æ®ä½  LangGraph çš„å…·ä½“ç‰ˆæœ¬ï¼ŒAPI å¯èƒ½æ˜¯ .stream æˆ– .astream
    async for event in graph.astream_events(
        {"messages": [("user", message)]}, 
        config=config,
        version="v1"
    ):
        # è¿‡æ»¤åªè¿”å› LLM ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡ä¸­é—´æ­¥éª¤çš„æ—¥å¿—
        if event["event"] == "on_chat_model_stream":
             content = event["data"]["chunk"].content
             if content:
                 yield content

# ==============================================================================
# 4. äº¤äº’å¼è¿è¡Œ
# ==============================================================================
def main():
    print("\nä½ å¯ä»¥å¼€å§‹æé—®äº† (è¾“å…¥ 'q' é€€å‡º)")
    
    # å®šä¹‰çº¿ç¨‹ IDï¼ŒLangGraph é€šè¿‡è¿™ä¸ª ID æ¥åŒºåˆ†ä¸åŒçš„å¯¹è¯å†å²
    # å¦‚æœä½ æƒ³å¼€å¯ä¸€æ®µå…¨æ–°çš„å¯¹è¯ï¼ˆå¿˜è®°è¿‡å»ï¼‰ï¼Œåªéœ€è¦æ¢ä¸€ä¸ª ID (ä¾‹å¦‚ "thread_2")
    config = {"configurable": {"thread_id": "factory_user_001"}}
    
    while True:
        user_input = input("\nè¯·æé—®: ")
        if user_input.lower() == 'q':
            break
            
        print("\n[Agent æ€è€ƒä¸­...]")

        # æˆ‘ä»¬åªæŠŠå½“å‰æœ€æ–°çš„è¿™ä¸€å¥è¯ä¼ ç»™ Agent
        # Agent ä¼šæ ¹æ® config é‡Œçš„ thread_id è‡ªåŠ¨å» memory é‡ŒæŸ¥æ‰¾ä¹‹å‰çš„èŠå¤©è®°å½•
        inputs = {"messages": [("user", user_input)]}
        
        # stream_mode="values" ä¼šè¿”å›å½“å‰æ—¶åˆ»å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«å†å²ï¼‰
        # æˆ‘ä»¬åªæ‰“å°æœ€åä¸€æ¡æ–°å¢çš„æ¶ˆæ¯
        for event in graph.stream(inputs, config=config, stream_mode="values"):
            last_message = event["messages"][-1]
            
            # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šåªæ‰“å° AI æ–°ç”Ÿæˆçš„å›å¤
            if last_message.type == "ai" and last_message.content:
                print(f"\n[åŠ©æ‰‹å›ç­”]: {last_message.content}")

if __name__ == "__main__":
    main()