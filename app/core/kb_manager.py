# app/core/kb_manager.py

import os
import shutil
import fitz  # PyMuPDF
import nest_asyncio
import requests
from typing import List, Dict,Optional
from fastapi import UploadFile
from llama_index.core import Document, VectorStoreIndex, StorageContext, Settings,SimpleDirectoryReader
from llama_index.vector_stores.elasticsearch import ElasticsearchStore
from app.core.agent import GLOBAL_EMBED_MODEL 
from dotenv import load_dotenv

load_dotenv(override=True)
nest_asyncio.apply()

ES_URL = "http://localhost:9200"
INDEX_NAME = "factory_knowledge"
UPLOAD_DIR = "./factory_docs"
IMAGES_DIR = "./factory_images"

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(IMAGES_DIR, exist_ok=True)

# -----------------------------------------------------------
# 1. æ ¸å¿ƒç®—æ³•ï¼šæŒ‰åæ ‡æå–å›¾æ–‡ï¼Œä¿æŒé¡ºåº
# -----------------------------------------------------------
def parse_pdf_with_layout(pdf_path: str, file_name: str) -> List[Document]:
    """
    ä½¿ç”¨ PyMuPDF è·å–é¡µé¢ä¸Šçš„æ–‡å­—å—å’Œå›¾ç‰‡å—ï¼Œå¹¶æ ¹æ® Y è½´åæ ‡è¿›è¡Œæ··åˆæ’åºã€‚
    è¿”å›åŒ…å«ç²¾ç¡®å›¾æ–‡é¡ºåºçš„ Document åˆ—è¡¨ã€‚
    """
    doc = fitz.open(pdf_path)
    base_name = os.path.splitext(file_name)[0]
    llama_documents = []

    print(f"ğŸ“„ å¼€å§‹è¿›è¡Œå›¾æ–‡æ··æ’è§£æ: {file_name}")

    for page_index, page in enumerate(doc):
        # 1. è·å–æ‰€æœ‰å›¾ç‰‡å¯¹è±¡
        image_list = page.get_images(full=True)
        page_items = [] # ç”¨äºå­˜æ”¾ (Yåæ ‡, å†…å®¹å­—ç¬¦ä¸²) çš„ä¸´æ—¶åˆ—è¡¨

        # --- A. å¤„ç†å›¾ç‰‡ ---
        for img_index, img in enumerate(image_list):
            xref = img[0]
            # è·å–å›¾ç‰‡åœ¨é¡µé¢ä¸Šçš„åæ ‡ (Rect)
            # æ³¨æ„ï¼šå¦‚æœä¸€å¼ å›¾è¢«å¤ç”¨å¤šæ¬¡ï¼Œget_image_rects ä¼šè¿”å›å¤šä¸ªä½ç½®ï¼Œè¿™é‡Œç®€åŒ–å–ç¬¬ä¸€ä¸ª
            rects = page.get_image_rects(xref)
            if not rects: 
                continue
            
            # è¿™é‡Œçš„ y1 (åº•éƒ¨åæ ‡) é€šå¸¸ç”¨äºå†³å®šå›¾ç‰‡æ˜¯åœ¨æŸæ®µæ–‡å­—ä¹‹å
            # æˆ‘ä»¬ç”¨ y0 (é¡¶éƒ¨åæ ‡) ä¹Ÿå¯ä»¥ï¼Œè§†æ’ç‰ˆè€Œå®šï¼Œé€šå¸¸ y0 æ›´ç¬¦åˆâ€œè¯»åˆ°è¿™é‡Œçœ‹åˆ°äº†å›¾â€
            y_pos = rects[0].y1 
            
            # æå–å›¾ç‰‡å¹¶ä¿å­˜åˆ°æœ¬åœ°
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]
            
            # æ–‡ä»¶åï¼šæ–‡ä»¶å_pé¡µç _ç´¢å¼•.png
            image_filename = f"{base_name}_p{page_index+1}_{img_index}.{image_ext}"
            image_path = os.path.join(IMAGES_DIR, image_filename)
            
            with open(image_path, "wb") as f:
                f.write(image_bytes)
            
            # æ„é€  Markdown å›¾ç‰‡é“¾æ¥
            # è¿™é‡Œç›´æ¥ç”Ÿæˆ URLï¼Œç¨åæ‹¼æ¥åˆ°æ–‡æœ¬é‡Œ
            img_url = f"http://localhost:8000/images/{image_filename}"
            markdown_img = f"\n\n![ç¤ºæ„å›¾]({img_url})\n\n"
            
            # å­˜å…¥åˆ—è¡¨: (åæ ‡, ç±»å‹, å†…å®¹)
            page_items.append({
                "y": y_pos,
                "type": "image",
                "content": markdown_img
            })

        # --- B. å¤„ç†æ–‡å­— ---
        # get_text("blocks") è¿”å› (x0, y0, x1, y1, "text", block_no, block_type)
        text_blocks = page.get_text("blocks")
        for block in text_blocks:
            # block[6] == 0 ä»£è¡¨è¿™æ˜¯æ–‡å­—å— (1æ˜¯å›¾ç‰‡å—ï¼Œä½†PyMuPDFçš„å›¾ç‰‡å—å¾€å¾€ä¸å‡†ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸Šé¢å•ç‹¬å¤„ç†äº†å›¾ç‰‡)
            if block[6] == 0:
                text_content = block[4].strip()
                if text_content:
                    page_items.append({
                        "y": block[3], # ä½¿ç”¨ y1 (åº•éƒ¨) ä½œä¸ºæ’åºä¾æ®
                        "type": "text",
                        "content": text_content
                    })

        # --- C. æ ¸å¿ƒï¼šæŒ‰ Y è½´åæ ‡æ’åº ---
        # è¿™æ ·å°±èƒ½ä¿è¯ï¼šä¸Šé¢çš„æ–‡å­— -> ä¸­é—´çš„å›¾ -> ä¸‹é¢çš„æ–‡å­—
        page_items.sort(key=lambda x: x["y"])

        # --- D. æ‹¼æ¥æˆæœ€ç»ˆæ–‡æœ¬ ---
        final_page_text = ""
        for item in page_items:
            final_page_text += item["content"] + "\n"

        # --- E. åˆ›å»º Document å¯¹è±¡ ---
        doc_obj = Document(text=final_page_text)
        doc_obj.metadata = {
            "file_name": file_name,
            "page_label": str(page_index + 1),
            # è¿™é‡Œè™½ç„¶æˆ‘ä»¬åœ¨texté‡Œå·²ç»åµŒå…¥äº†å›¾ç‰‡ï¼Œä½†metadataé‡Œç•™ä¸ªåº•ä¹Ÿæ˜¯å¥½çš„
            "has_images": True if image_list else False 
        }
        llama_documents.append(doc_obj)

    print(f"âœ… è§£æå®Œæˆï¼Œå…± {len(llama_documents)} é¡µ")
    return llama_documents

# -----------------------------------------------------------
# 2. ES æ“ä½œå‡½æ•°
# -----------------------------------------------------------
def list_files_in_es() -> List[Dict]:
    search_url = f"{ES_URL}/{INDEX_NAME}/_search"
    payload = {
        "size": 0, "aggs": {"unique_files": {"terms": {"field": "metadata.file_name.keyword", "size": 1000}}}
    }
    try:
        response = requests.get(search_url, json=payload)
        if response.status_code == 200:
            buckets = response.json().get('aggregations', {}).get('unique_files', {}).get('buckets', [])
            return [{"name": b['key'], "chunks": b['doc_count']} for b in buckets]
        return []
    except Exception as e:
        return []

def delete_file_from_es(filename: str) -> bool:
    url = f"{ES_URL}/{INDEX_NAME}/_delete_by_query"
    payload = {"query": {"term": {"metadata.file_name.keyword": filename}}}
    try:
        response = requests.post(url, json=payload)
        return response.status_code == 200
    except:
        return False

# -----------------------------------------------------------
# 3. å…¥åº“å…¥å£
# -----------------------------------------------------------
# æ–°å¢ï¼šé€šç”¨å…¥åº“é€»è¾‘ï¼ˆæ¥æ”¶æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼‰
async def ingest_from_local_path(file_path: str, original_filename: str):
    print(f"ğŸ“‚ å¼€å§‹å¤„ç†æœ¬åœ°æ–‡ä»¶: {original_filename}")

    # 1. è§£ææ–‡æ¡£
    documents = []
    if original_filename.lower().endswith(".pdf"):
        documents = parse_pdf_with_layout(file_path, original_filename)
    else:
        # å¯¹äº txt, md, docx ç­‰ï¼Œä½¿ç”¨ SimpleDirectoryReader
        documents = SimpleDirectoryReader(input_files=[file_path]).load_data()
        # ç¡®ä¿ metadata é‡Œæœ‰æ–‡ä»¶å
        for doc in documents:
            doc.metadata["file_name"] = original_filename
            doc.metadata["page_label"] = "1" # éPDFé»˜è®¤ä¸ºç¬¬1é¡µ

    # 2. æ˜¾å­˜ä¿æŠ¤é…ç½®
    Settings.embed_model = GLOBAL_EMBED_MODEL
    Settings.chunk_size = 512

    # 3. å­˜å…¥ ES
    print(f"â³ å¼€å§‹å‘é‡åŒ–å…¥åº“ ({len(documents)} ä¸ªç‰‡æ®µ)...")
    vector_store = ElasticsearchStore(
        es_url=ES_URL,
        index_name=INDEX_NAME,
    )
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    VectorStoreIndex.from_documents(
        documents,
        storage_context=storage_context,
        show_progress=True
    )
    
    print(f"ğŸ‰ {original_filename} å…¥åº“å®Œæˆï¼")
    return len(documents)

# å¤„ç†ä¸Šä¼ æ–‡ä»¶
async def ingest_file(file: UploadFile):
    # 1. ä¿å­˜æ–‡ä»¶åˆ°ç£ç›˜
    file_path = os.path.join(UPLOAD_DIR, file.filename)
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    # 2. è°ƒç”¨é€šç”¨é€»è¾‘
    return await ingest_from_local_path(file_path, file.filename)