# app/core/kb_manager.py

import os
import shutil
import fitz  # PyMuPDF
import requests
import nest_asyncio
from typing import List, Dict
from fastapi import UploadFile
from llama_parse import LlamaParse
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings
from llama_index.vector_stores.elasticsearch import ElasticsearchStore
from app.core.agent import GLOBAL_EMBED_MODEL 
# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv(override=True)


# åº”ç”¨å¼‚æ­¥è¡¥ä¸
nest_asyncio.apply()

ES_URL = "http://localhost:9200"
INDEX_NAME = "factory_knowledge"
UPLOAD_DIR = "./factory_docs"       # å­˜æ”¾ PDF åŸæ–‡
IMAGES_DIR = "./factory_images"     # å­˜æ”¾æŠ å‡ºæ¥çš„å›¾ç‰‡

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(IMAGES_DIR, exist_ok=True)

# -----------------------------------------------------------
# 1. è¾…åŠ©å‡½æ•°ï¼šæœ¬åœ°æš´åŠ›æå– PDF å›¾ç‰‡
# -----------------------------------------------------------
def extract_images_from_pdf(pdf_path, output_dir):
    """
    ä½¿ç”¨ PyMuPDF ä» PDF ä¸­æå–æ‰€æœ‰å›¾ç‰‡ï¼Œå¹¶è¿”å›å›¾ç‰‡çš„æ–‡ä»¶ååˆ—è¡¨ã€‚
    """
    image_files = []
    try:
        doc = fitz.open(pdf_path)
        base_name = os.path.splitext(os.path.basename(pdf_path))[0]
        
        print(f"ğŸ–¼ï¸  å¼€å§‹ä» {base_name} ä¸­æå–å›¾ç‰‡...")
        
        for i in range(len(doc)):
            page = doc[i]
            image_list = page.get_images(full=True)
            
            if image_list:
                print(f"    - ç¬¬ {i+1} é¡µå‘ç° {len(image_list)} å¼ å›¾ç‰‡")
            
            for img_index, img in enumerate(image_list):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                image_ext = base_image["ext"]  # png æˆ– jpeg
                
                # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å: æ–‡ä»¶å_é¡µç _å›¾ç‰‡ç´¢å¼•.png
                image_filename = f"{base_name}_p{i+1}_{img_index}.{image_ext}"
                image_filepath = os.path.join(output_dir, image_filename)
                
                # ä¿å­˜å›¾ç‰‡åˆ°ç¡¬ç›˜
                with open(image_filepath, "wb") as f:
                    f.write(image_bytes)
                
                image_files.append(image_filename)
                
        print(f"âœ… å›¾ç‰‡æå–å®Œæˆï¼Œå…± {len(image_files)} å¼ ï¼Œå­˜å…¥ {output_dir}")
        return image_files
    except Exception as e:
        print(f"âŒ å›¾ç‰‡æå–å¤±è´¥: {e}")
        return []

# -----------------------------------------------------------
# 2. ES æ“ä½œå‡½æ•°
# -----------------------------------------------------------
def list_files_in_es() -> List[Dict]:
    search_url = f"{ES_URL}/{INDEX_NAME}/_search"
    payload = {
        "size": 0,
        "aggs": {
            "unique_files": {
                "terms": {
                    "field": "metadata.file_name.keyword",
                    "size": 1000
                }
            }
        }
    }
    try:
        response = requests.get(search_url, json=payload)
        if response.status_code == 200:
            buckets = response.json().get('aggregations', {}).get('unique_files', {}).get('buckets', [])
            return [{"name": b['key'], "chunks": b['doc_count']} for b in buckets]
        return []
    except Exception as e:
        print(f"æŸ¥è¯¢å¤±è´¥: {e}")
        return []

def delete_file_from_es(filename: str) -> bool:
    url = f"{ES_URL}/{INDEX_NAME}/_delete_by_query"
    payload = {
        "query": {
            "term": {
                "metadata.file_name.keyword": filename
            }
        }
    }
    try:
        response = requests.post(url, json=payload)
        return response.status_code == 200
    except Exception as e:
        print(f"åˆ é™¤å¤±è´¥: {e}")
        return False

# -----------------------------------------------------------
# 3. æ ¸å¿ƒå…¥åº“é€»è¾‘
# -----------------------------------------------------------
async def ingest_file(file: UploadFile):
    # 1. ä¿å­˜ PDF åŸæ–‡ä»¶
    file_path = os.path.join(UPLOAD_DIR, file.filename)
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    print(f"ğŸ“‚ å¤„ç†æ–‡ä»¶: {file.filename}")

    # 2. å…ˆè¿›è¡Œæœ¬åœ°å›¾ç‰‡æå–
    extracted_images = []
    if file.filename.lower().endswith(".pdf"):
        extracted_images = extract_images_from_pdf(file_path, IMAGES_DIR)

    # 3. é…ç½® LlamaParse
    
    parser = LlamaParse(
        api_key=os.getenv('LLAMA_CLOUD_API_KEY'),
        result_type="markdown",
        language="ch_sim",
        verbose=True,
        premium_mode=False, 
        take_screenshot=False,
        split_by_page=True,
    )
    
    file_extractor = {".pdf": parser, ".docx": parser, ".doc": parser}

    # 4. ä½¿ç”¨å…¨å±€æ¨¡å‹
    Settings.embed_model = GLOBAL_EMBED_MODEL
    Settings.chunk_size = 512

    # 5. è¯»å–å¹¶è§£ææ–‡å­—
    documents = SimpleDirectoryReader(
        input_files=[file_path],
        file_extractor=file_extractor
    ).load_data()

    # 6. ç²¾å‡†åˆ†é…å›¾ç‰‡åˆ°æ¯ä¸€é¡µ
    if extracted_images:
        print(f"ğŸ”— æ­£åœ¨å°†å›¾ç‰‡ç²¾ç¡®åŒ¹é…åˆ°å¯¹åº”é¡µç ...")
        
        for doc in documents:
            # LlamaParse ä¼šè‡ªåŠ¨åœ¨ metadata é‡Œæ”¾å…¥ 'page_label' (é€šå¸¸æ˜¯ "1", "2" å­—ç¬¦ä¸²)
            page_label = doc.metadata.get("page_label")
            
            if page_label:
                # æ„é€ åŒ¹é…ç‰¹å¾ï¼Œä¾‹å¦‚ "_p1_" (å¯¹åº”ç¬¬1é¡µ)
                # æˆ‘ä»¬çš„å›¾ç‰‡å‘½åæ ¼å¼æ˜¯: base_name_p{é¡µç }_{ç´¢å¼•}.ext
                match_str = f"_p{page_label}_"
                
                # ç­›é€‰å±äºè¿™ä¸€é¡µçš„å›¾ç‰‡
                page_images = [img for img in extracted_images if match_str in img]
                
                # åªæŠŠè¿™ä¸€é¡µçš„å›¾ç‰‡æŒ‚è½½åˆ°å½“å‰æ–‡æ¡£
                if page_images:
                    doc.metadata["image_files"] = page_images
            else:
                # å¦‚æœæ˜¯ Word/Excel æ²¡æœ‰é¡µç æ¦‚å¿µï¼Œæˆ–è€… LlamaParse æ²¡è¿”å›é¡µç 
                # å¯ä»¥é€‰æ‹©æŒ‚è½½æ‰€æœ‰å›¾ç‰‡ï¼Œæˆ–è€…ä¸æŒ‚è½½
                pass

    # 7. å­˜å…¥ ES
    vector_store = ElasticsearchStore(
        es_url=ES_URL,
        index_name=INDEX_NAME,
    )
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    print(f"â³ å‡†å¤‡å¼€å§‹å‘é‡åŒ–ï¼Œå…±æœ‰ {len(documents)} ä¸ªæ–‡æ¡£å¯¹è±¡ç­‰å¾…å¤„ç†...")
    print("   (è¿™ä¸€æ­¥éœ€è¦è°ƒç”¨æ˜¾å¡ BGE-M3 æ¨¡å‹ï¼Œå¦‚æœæ–‡æ¡£å¾ˆé•¿ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...)")
    try:
        # è¿™é‡Œæ˜¯æœ€å®¹æ˜“å¡ä½çš„åœ°æ–¹
        index = VectorStoreIndex.from_documents(
            documents,
            storage_context=storage_context,
            show_progress=True  # å¼€å¯å†…ç½®è¿›åº¦æ¡ï¼
        )
        print("âœ… å‘é‡åŒ–å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ å‘é‡åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise e

    print("ğŸ‰ å…¨é‡å…¥åº“å®Œæˆï¼")
    
    return len(documents)